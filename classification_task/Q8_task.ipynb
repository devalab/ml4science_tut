{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Secondary Structure Prediction with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Class secondary structure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import torch.utils.data as utils\n",
    "from torch.nn.utils.rnn import pad_sequence ,pack_padded_sequence,pad_packed_sequence\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number Initializers for Repeatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names=['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T']\n",
    "residue_names = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set device for the model to run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "    return var\n",
    "\n",
    "# class to index loaded dataset\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, path_to_pkl):\n",
    "        self.data = load_from_pkl(path_to_pkl)\n",
    "        self.ids = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        dict = self.data[id]\n",
    "        seq = dict[\"sequence\"]\n",
    "        pssm = dict[\"pssm\"]\n",
    "        length = len(seq)\n",
    "        \n",
    "        secstr = dict[\"secstr\"]\n",
    "        secstr_q3 = dict[\"secstr_q3\"]\n",
    "\n",
    "        return id,seq,pssm,length,secstr,secstr_q3\n",
    "    \n",
    "# Helper Function to pad a batch of protein sequences.\n",
    "def pad_batch(DataLoaderBatch):\n",
    "    \"\"\"\n",
    "    DataLoaderBatch should be a list of (sequence, target, length) tuples...\n",
    "    Returns a padded tensor of sequences sorted from longest to shortest, \n",
    "    \"\"\"\n",
    "    batch_size = len(DataLoaderBatch)\n",
    "    batch_split = list(zip(*DataLoaderBatch))\n",
    "\n",
    "    ids, seqs, pssms, lengths, secstrs, secstrs_q3 = batch_split[0], batch_split[1], batch_split[2] ,batch_split[3], batch_split[4], batch_split[5]\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    padded_seqs = np.ones((max_length, batch_size))*30\n",
    "    padded_pssms = np.zeros((max_length,batch_size,22))\n",
    "    padded_secstrs = np.ones((max_length,batch_size))*30\n",
    "    padded_secstrs_q3 = np.ones((max_length,batch_size))*30\n",
    "    \n",
    "    for i, l in enumerate(lengths):\n",
    "        padded_seqs[:l,i] = seqs[i][:l]\n",
    "        padded_pssms[:l,i,:] = pssms[i][:l][:]\n",
    "        padded_secstrs[:l,i] = secstrs[i][:l]\n",
    "        padded_secstrs_q3[:l,i] = secstrs_q3[i][:l]\n",
    "    return torch.tensor(padded_seqs).type(torch.LongTensor), torch.tensor(padded_pssms).type(torch.float), torch.tensor(lengths), torch.tensor(padded_secstrs).type(torch.LongTensor), torch.tensor(padded_secstrs_q3).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for data parallelism\n",
    "batch_size = 4\n",
    "\n",
    "# Load train, test and validation data\n",
    "train = Dataset(\"./Datasets/train.pkl\")\n",
    "val = Dataset(\"./Datasets/val.pkl\")\n",
    "test = Dataset(\"./Datasets/test.pkl\")\n",
    "\n",
    "# Data Generators for each set.\n",
    "train_gen = data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=pad_batch,num_workers=1)\n",
    "val_gen = data.DataLoader(val, batch_size=batch_size,shuffle=False,collate_fn=pad_batch,num_workers=1)\n",
    "test_gen = data.DataLoader(test, batch_size=batch_size,shuffle=False,collate_fn=pad_batch,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper module for perceptron layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,dropout = 0.2,normalization = True,activation = F.relu):\n",
    "        super(Dense, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout\n",
    "        self.activation = activation\n",
    "        self.n_state = normalization        \n",
    "        self.linear = nn.Linear(self.input_dim,self.output_dim)\n",
    "        self.layernorm = nn.LayerNorm(self.output_dim)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        if self.n_state:\n",
    "            x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper module for Bidirectional GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional GRU\n",
    "class gru_bidirectional(nn.Module):\n",
    "    def __init__(self,inp,out,dropout=0.1,activation=F.relu):\n",
    "        super(gru_bidirectional,self).__init__()\n",
    "        self.gru = nn.GRU(inp,out,bidirectional=True)\n",
    "        self.dropout_rate = dropout\n",
    "        self.layernorm = nn.LayerNorm(2*out)\n",
    "        self.activation = activation\n",
    "        self.input_dim = inp\n",
    "        self.output_dim = out\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self,x,lengths):\n",
    "        x_ = pack_padded_sequence(x,lengths,enforce_sorted=False)\n",
    "        out,_ = self.gru(x_)\n",
    "        out,_ = pad_packed_sequence(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.layernorm(out)\n",
    "        if self.input_dim == 2*self.output_dim:\n",
    "            out = out+x\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural network definition\n",
    "\n",
    "The structure of the neural network is defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        '''\n",
    "        Fill here\n",
    "        '''\n",
    "        \n",
    "    def forward(self,x,pssm,x_lengths):\n",
    "        '''\n",
    "        Forward Pass here\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the network, Optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=30)\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Epoch helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(generator):\n",
    "    #     Set the network in to train mode\n",
    "    #     Enables dropout. \n",
    "    network.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time() # Track training time.\n",
    "    prediction = []\n",
    "    g_t = []\n",
    "    for seq,pssm,length,q8,q3 in (generator):\n",
    "        '''\n",
    "        Fill Training steps here\n",
    "        \n",
    "        variable loss should contain loss\n",
    "        variable output should contain predictions generated by the network in the format [length,batch_size,8]\n",
    "        '''\n",
    "        total_loss += loss.cpu().detach().numpy()/batch_size   # Track loss for analysis\n",
    "        \n",
    "        # store all predictions to calculate accuracy at end of epoch\n",
    "        for i in range(len(length)):\n",
    "            prediction.append(output[0:length[i],i,:].cpu().detach().numpy())\n",
    "            g_t.append(q8[0:length[i],i].cpu().detach().numpy())\n",
    "\n",
    "    predictions = np.argmax(np.concatenate(prediction,axis=0),axis=-1)\n",
    "    g_ts = np.concatenate(g_t)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(predictions,g_ts) \n",
    "    end_time = time.time()\n",
    "    return end_time-start_time , total_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Epoch Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(generator, print_acc_report=False,output_sample=False):\n",
    "    \n",
    "    with torch.no_grad(): # disable gradients as no optimization done during evaluation\n",
    "        network.eval()  # set network to evaluation mode to disable dropouts\n",
    "        total_loss = 0.\n",
    "        start_time = time.time() # Track evaluation time\n",
    "        prediction = []\n",
    "        g_t = []   \n",
    "        for seq,pssm,length,q8,q3 in (generator):\n",
    "            \n",
    "            #    Get predictions\n",
    "            output = network(seq.to(device),pssm.to(device),length.to(device))\n",
    "            \n",
    "            #    Calculate loss for analysis\n",
    "            loss = criterion(output.view(-1,8).to(device),q8.view(-1).to(device))        \n",
    "            total_loss += loss.cpu().detach().numpy()/batch_size\n",
    "            for i in range(len(length)):\n",
    "                prediction.append(output[0:length[i],i,:].cpu().detach().numpy())\n",
    "                g_t.append(q8[0:length[i],i].cpu().detach().numpy())\n",
    "                \n",
    "        #   calculate  accuracy for analysis       \n",
    "        predictions = np.argmax(np.concatenate(prediction,axis=0),axis=-1)\n",
    "        g_ts = np.concatenate(g_t)\n",
    "        acc = accuracy_score(predictions,g_ts) \n",
    "        end_time = time.time()\n",
    "        if print_acc_report:\n",
    "            print (classification_report(g_ts,predictions,target_names=target_names))\n",
    "    return end_time-start_time, total_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "for i in range(40):\n",
    "    #     Train epoch\n",
    "    run_time,train_loss,train_acc = train_epoch(train_gen)\n",
    "    \n",
    "    #     Validation step\n",
    "    _,val_loss,val_acc = evaluate(val_gen)\n",
    "    \n",
    "    #     Track performance of network during training\n",
    "    train_acc_history.append(train_acc)\n",
    "    val_acc_history.append(val_acc)\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    \n",
    "    #     Print performance metrics\n",
    "    print (\"Epoch : \"+str(i)+\", Train_loss : \"+str(train_loss)+\", Train_acc : \"+str(train_acc)+\", Val_acc : \"+ str(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,test_accuracy = evaluate(test_gen,True,True)\n",
    "print (\"Test Accuracy - \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_history,label = \"Train_accuracy\")\n",
    "plt.plot(val_acc_history,label = \"Validation_accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.title(\"Performance with time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss with time\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
